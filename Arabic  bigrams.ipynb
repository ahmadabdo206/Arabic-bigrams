{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('Text Document.txt', 'r', encoding='utf-8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©',\n",
       " '',\n",
       " 'ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§',\n",
       " 'Ø§Ø¨Ø­Ø« ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§',\n",
       " 'Ø¨Ø­Ø«',\n",
       " 'Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø³Ø§Ø¨',\n",
       " 'Ø¯Ø®ÙˆÙ„',\n",
       " '',\n",
       " 'Ø£Ø¯ÙˆØ§Øª Ø´Ø®ØµÙŠØ©',\n",
       " '\\t',\n",
       " 'ØªØ¶Ø§Ù…Ù†Ø§ Ù…Ø¹ Ø­Ù‚ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ',\n",
       " 'Ù„Ø§ Ù„Ù„Ø¥Ø¨Ø§Ø¯Ø© Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ© ÙÙŠ ØºØ²Ø© .... Ù„Ø§ Ù„Ù‚ØªÙ„ Ø§Ù„Ù…Ø¯Ù†ÙŠÙŠÙ†',\n",
       " 'Ù„Ø§ Ù„Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø§Ù„Ù…Ø³ØªØ´ÙÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¯Ø§Ø±Ø³ .... Ù„Ø§ Ù„Ù„ØªØ¶Ù„ÙŠÙ„ ÙˆØ§Ù„ÙƒÙŠÙ„ Ø¨Ù…ÙƒÙŠØ§Ù„ÙŠÙ†',\n",
       " 'Ø£ÙˆÙ‚ÙÙˆØ§ Ø§Ù„Ø­Ø±Ø¨ .... ÙˆØ§Ù†Ø´Ø±ÙˆØ§ Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ø¹Ø§Ø¯Ù„ ÙˆØ§Ù„Ø´Ø§Ù…Ù„',\n",
       " '',\n",
       " 'Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³Ø©',\n",
       " 'Ù†Ù‚Ø§Ø´',\n",
       " 'Ø§Ù‚Ø±Ø£',\n",
       " 'Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ¯Ø±']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '%',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " 'Â«',\n",
       " 'Â®',\n",
       " 'Â²',\n",
       " 'Â·',\n",
       " 'Â»',\n",
       " 'Ã©',\n",
       " 'Ã³',\n",
       " 'Ã´',\n",
       " 'Î­',\n",
       " 'Î±',\n",
       " 'Î³',\n",
       " 'Îµ',\n",
       " 'Î¹',\n",
       " 'Îº',\n",
       " 'Î»',\n",
       " 'Î½',\n",
       " 'Î¿',\n",
       " 'Ï€',\n",
       " 'Ï‚',\n",
       " 'Ïƒ',\n",
       " 'Ï„',\n",
       " 'Ï…',\n",
       " 'ÏŒ',\n",
       " 'Ğ’',\n",
       " 'Ğ ',\n",
       " 'Ğ¡',\n",
       " 'Ğ¸',\n",
       " 'Ğ¾',\n",
       " 'Ñ',\n",
       " 'Öµ',\n",
       " 'Ö·',\n",
       " 'Ö¼',\n",
       " '×',\n",
       " '×',\n",
       " '×”',\n",
       " '×•',\n",
       " '×™',\n",
       " '×¢',\n",
       " '×©',\n",
       " 'ØŒ',\n",
       " 'Ø›',\n",
       " 'ØŸ',\n",
       " 'Ø¡',\n",
       " 'Ø¢',\n",
       " 'Ø£',\n",
       " 'Ø¤',\n",
       " 'Ø¥',\n",
       " 'Ø¦',\n",
       " 'Ø§',\n",
       " 'Ø¨',\n",
       " 'Ø©',\n",
       " 'Øª',\n",
       " 'Ø«',\n",
       " 'Ø¬',\n",
       " 'Ø­',\n",
       " 'Ø®',\n",
       " 'Ø¯',\n",
       " 'Ø°',\n",
       " 'Ø±',\n",
       " 'Ø²',\n",
       " 'Ø³',\n",
       " 'Ø´',\n",
       " 'Øµ',\n",
       " 'Ø¶',\n",
       " 'Ø·',\n",
       " 'Ø¸',\n",
       " 'Ø¹',\n",
       " 'Øº',\n",
       " 'Ù',\n",
       " 'Ù‚',\n",
       " 'Ùƒ',\n",
       " 'Ù„',\n",
       " 'Ù…',\n",
       " 'Ù†',\n",
       " 'Ù‡',\n",
       " 'Ùˆ',\n",
       " 'Ù‰',\n",
       " 'ÙŠ',\n",
       " 'Ù ',\n",
       " 'Ù¡',\n",
       " 'Ù¬',\n",
       " 'Ù±',\n",
       " 'Ù¾',\n",
       " 'Ú¤',\n",
       " 'ÛŒ',\n",
       " 'Ü˜',\n",
       " 'Ü',\n",
       " 'Ü¥',\n",
       " 'Ü«',\n",
       " 'İ ',\n",
       " 'à¤—',\n",
       " 'à¤¤',\n",
       " 'à¤¥',\n",
       " 'à¤¦',\n",
       " 'à¤§',\n",
       " 'à¤®',\n",
       " 'à¤°',\n",
       " 'à¤¸',\n",
       " 'à¤¾',\n",
       " 'à¤¿',\n",
       " 'à¥Œ',\n",
       " 'à¥',\n",
       " 'á½',\n",
       " 'á½',\n",
       " '\\u200c',\n",
       " '\\u200f',\n",
       " 'â€“',\n",
       " 'â€”',\n",
       " 'â€œ',\n",
       " 'â€',\n",
       " 'â€¢',\n",
       " 'â†’',\n",
       " 'â†“',\n",
       " 'â‡§',\n",
       " 'âœ',\n",
       " 'ğŸ‘ˆ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = []\n",
    "with open('New Text Document (2).txt', 'r', encoding='utf-8') as sentences:\n",
    "  for line in sentences:\n",
    "    for char in line:\n",
    "      characters.append(char)\n",
    "characters=sorted(list(set(characters)))\n",
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ar_characters=['Ø¡','Ø¢','Ø£','Ø¤', 'Ø¥', 'Ø¦', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´'\n",
    "               , 'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'Ù‰', 'ÙŠ', 'Ù±']\n",
    "len(Ar_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6055,\n",
       " ['Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©',\n",
       "  'Ø§Ø¨Ø­Ø«',\n",
       "  'ÙÙŠ',\n",
       "  'Ø¨Ø­Ø«',\n",
       "  'Ø¥Ù†Ø´Ø§Ø¡',\n",
       "  'Ø­Ø³Ø§Ø¨',\n",
       "  'Ø¯Ø®ÙˆÙ„',\n",
       "  'Ø£Ø¯ÙˆØ§Øª',\n",
       "  'Ø´Ø®ØµÙŠØ©',\n",
       "  'ØªØ¶Ø§Ù…Ù†Ø§',\n",
       "  'Ù…Ø¹',\n",
       "  'Ø­Ù‚',\n",
       "  'Ø§Ù„Ø´Ø¹Ø¨',\n",
       "  'Ù„Ø§',\n",
       "  'Ù„Ù„Ø¥Ø¨Ø§Ø¯Ø©',\n",
       "  'ØºØ²Ø©',\n",
       "  'Ù„Ù‚ØªÙ„',\n",
       "  'Ù„Ù„ØªØ¶Ù„ÙŠÙ„',\n",
       "  'ÙˆØ§Ù„ÙƒÙŠÙ„',\n",
       "  'Ø£ÙˆÙ‚ÙÙˆØ§'],\n",
       " ['Ø§Ù„ÙƒÙˆÙ†ØºÙˆ',\n",
       "  'ØªÙˆÙ…ÙŠ',\n",
       "  'Ø´Ø¹ÙˆØ¨',\n",
       "  'Ø£Ù†ØªÙŠØºÙˆØ§',\n",
       "  'ÙƒÙŠØªØ³',\n",
       "  'ÙÙ†Ø³Ù†Øª',\n",
       "  'ÙˆØªÙˆØ¨Ø§ØºÙˆ',\n",
       "  'Ø¨ÙˆÙŠÙ†Ø³',\n",
       "  'Ø¢ÙŠØ±Ø³',\n",
       "  'Ø¢Ø´ÙˆØ±ÙŠÙˆÙ†',\n",
       "  'Ø³Ø±ÙŠØ§Ù†',\n",
       "  'ÙƒÙ„Ø¯Ø§Ù†',\n",
       "  'Ø¨Ù„Ø¯Ø§Ù†',\n",
       "  'ÙˆØ£Ù‚Ø§Ù„ÙŠÙ…',\n",
       "  'Ø¬Ø²Ø±',\n",
       "  'Ù…Ø³Ø§Ø±Ø¯',\n",
       "  'ÙˆØªØ±Ø§Ø¬Ù…',\n",
       "  'Ø£Ø¨Ø¬Ø¯ÙŠ',\n",
       "  'Ø±Ø¦ÙŠØ³ÙŠØ©'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_words(Old_File, New_File):\n",
    "    with open(Old_File, 'r', encoding='utf-8') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    filtered_sentences = []\n",
    "    for line in sentences:\n",
    "        \n",
    "        words = line.split()\n",
    "        \n",
    "        filtered_words = [word for word in words if all(char in Ar_characters for char in word)]\n",
    "        filtered_sentences.append(filtered_words)\n",
    "    all_words = []    \n",
    "    for  line in filtered_sentences:\n",
    "        for words in line:\n",
    "            if len(words)<=7:\n",
    "                if words not in all_words:\n",
    "                    all_words.append(words)\n",
    "                    \n",
    "    for word in all_words:\n",
    "        with open(New_File, 'a', encoding='utf-8') as out_file:\n",
    "            out_file.write(word + '\\n')\n",
    "      \n",
    "    return(all_words)\n",
    "    \n",
    "\n",
    "            \n",
    "new_words = filter_words('Text Document.txt','F:\\\\Arabic_names.txt')\n",
    "len(new_words),new_words[:20],new_words[-20:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('F:\\\\IE\\\\Andrej Karpathy\\\\1\\\\Ar_names.txt', 'r', encoding='utf-8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©',\n",
       " 'Ø§Ø¨Ø­Ø«',\n",
       " 'ÙÙŠ',\n",
       " 'Ø¨Ø­Ø«',\n",
       " 'Ø¥Ù†Ø´Ø§Ø¡',\n",
       " 'Ø­Ø³Ø§Ø¨',\n",
       " 'Ø¯Ø®ÙˆÙ„',\n",
       " 'Ø£Ø¯ÙˆØ§Øª',\n",
       " 'Ø´Ø®ØµÙŠØ©',\n",
       " 'ØªØ¶Ø§Ù…Ù†Ø§',\n",
       " 'Ù…Ø¹',\n",
       " 'Ø­Ù‚',\n",
       " 'Ø§Ù„Ø´Ø¹Ø¨',\n",
       " 'Ù„Ø§',\n",
       " 'Ù„Ù„Ø¥Ø¨Ø§Ø¯Ø©',\n",
       " 'ØºØ²Ø©',\n",
       " 'Ù„Ù‚ØªÙ„',\n",
       " 'Ù„Ù„ØªØ¶Ù„ÙŠÙ„',\n",
       " 'ÙˆØ§Ù„ÙƒÙŠÙ„',\n",
       " 'Ø£ÙˆÙ‚ÙÙˆØ§']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239471"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ø¡': 1,\n",
       " 'Ø¢': 2,\n",
       " 'Ø£': 3,\n",
       " 'Ø¤': 4,\n",
       " 'Ø¥': 5,\n",
       " 'Ø¦': 6,\n",
       " 'Ø§': 7,\n",
       " 'Ø¨': 8,\n",
       " 'Ø©': 9,\n",
       " 'Øª': 10,\n",
       " 'Ø«': 11,\n",
       " 'Ø¬': 12,\n",
       " 'Ø­': 13,\n",
       " 'Ø®': 14,\n",
       " 'Ø¯': 15,\n",
       " 'Ø°': 16,\n",
       " 'Ø±': 17,\n",
       " 'Ø²': 18,\n",
       " 'Ø³': 19,\n",
       " 'Ø´': 20,\n",
       " 'Øµ': 21,\n",
       " 'Ø¶': 22,\n",
       " 'Ø·': 23,\n",
       " 'Ø¸': 24,\n",
       " 'Ø¹': 25,\n",
       " 'Øº': 26,\n",
       " 'Ù': 27,\n",
       " 'Ù‚': 28,\n",
       " 'Ùƒ': 29,\n",
       " 'Ù„': 30,\n",
       " 'Ù…': 31,\n",
       " 'Ù†': 32,\n",
       " 'Ù‡': 33,\n",
       " 'Ùˆ': 34,\n",
       " 'Ù‰': 35,\n",
       " 'ÙŠ': 36,\n",
       " 'Ù±': 37,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Ø¡',\n",
       " 2: 'Ø¢',\n",
       " 3: 'Ø£',\n",
       " 4: 'Ø¤',\n",
       " 5: 'Ø¥',\n",
       " 6: 'Ø¦',\n",
       " 7: 'Ø§',\n",
       " 8: 'Ø¨',\n",
       " 9: 'Ø©',\n",
       " 10: 'Øª',\n",
       " 11: 'Ø«',\n",
       " 12: 'Ø¬',\n",
       " 13: 'Ø­',\n",
       " 14: 'Ø®',\n",
       " 15: 'Ø¯',\n",
       " 16: 'Ø°',\n",
       " 17: 'Ø±',\n",
       " 18: 'Ø²',\n",
       " 19: 'Ø³',\n",
       " 20: 'Ø´',\n",
       " 21: 'Øµ',\n",
       " 22: 'Ø¶',\n",
       " 23: 'Ø·',\n",
       " 24: 'Ø¸',\n",
       " 25: 'Ø¹',\n",
       " 26: 'Øº',\n",
       " 27: 'Ù',\n",
       " 28: 'Ù‚',\n",
       " 29: 'Ùƒ',\n",
       " 30: 'Ù„',\n",
       " 31: 'Ù…',\n",
       " 32: 'Ù†',\n",
       " 33: 'Ù‡',\n",
       " 34: 'Ùˆ',\n",
       " 35: 'Ù‰',\n",
       " 36: 'ÙŠ',\n",
       " 37: 'Ù±',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 38)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stoi), len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((38, 38), dtype=torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for chr in words:\n",
    "  chs = ['.'] + list(chr) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    N[ix1, ix2] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,   884,  8632,     0,  2886,     0, 55780, 14411,     5,\n",
       "        15938,  1452,  4756,  6066,  2212,  3428,   769,  4381,  1192,  5915,\n",
       "         3308,  2571,   437,  2175,   306,  8084,  1619,  7920,  4425,  5943,\n",
       "        11345, 24361,  5061,  3309, 23152,     5,  6613,   130],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0036),\n",
       " tensor(0.0036),\n",
       " tensor(0.0068),\n",
       " tensor(0.0347),\n",
       " tensor(0.0036),\n",
       " tensor(0.0140),\n",
       " tensor(0.0036),\n",
       " tensor(0.2048),\n",
       " tensor(0.0556),\n",
       " tensor(0.0036),\n",
       " tensor(0.0611),\n",
       " tensor(0.0088),\n",
       " tensor(0.0207),\n",
       " tensor(0.0255),\n",
       " tensor(0.0116),\n",
       " tensor(0.0160),\n",
       " tensor(0.0064),\n",
       " tensor(0.0194),\n",
       " tensor(0.0079),\n",
       " tensor(0.0249),\n",
       " tensor(0.0155),\n",
       " tensor(0.0129),\n",
       " tensor(0.0052),\n",
       " tensor(0.0114),\n",
       " tensor(0.0047),\n",
       " tensor(0.0327),\n",
       " tensor(0.0094),\n",
       " tensor(0.0322),\n",
       " tensor(0.0195),\n",
       " tensor(0.0250),\n",
       " tensor(0.0445),\n",
       " tensor(0.0914),\n",
       " tensor(0.0218),\n",
       " tensor(0.0155),\n",
       " tensor(0.0871),\n",
       " tensor(0.0036),\n",
       " tensor(0.0274),\n",
       " tensor(0.0041)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = (N+995).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "list(P[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999403953552"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1].sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù†Ù‚Ø·Ø©.\n",
      "Ù….\n",
      "ØªØ§Ù„Ø£Ø±Ø©.\n",
      "ØªØµ.\n",
      "Ù…Ø§Ù„Ø¹Ù†Ø¯.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    p = N[ix].float()\n",
    "    p = p / p.sum()\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  1453893\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((38, 38), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch 4.235983371734619\n",
      "5 epoch 3.282212972640991\n",
      "10 epoch 3.0958139896392822\n",
      "15 epoch 3.0035343170166016\n",
      "20 epoch 2.9488487243652344\n",
      "25 epoch 2.9133543968200684\n",
      "30 epoch 2.888604164123535\n",
      "35 epoch 2.8705005645751953\n",
      "40 epoch 2.856851100921631\n",
      "45 epoch 2.8463404178619385\n",
      "Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: 113.14499235153198 Ø«Ø§Ù†ÙŠØ©\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ø¨Ø¯Ø¡ Ù‚Ø¨Ù„ ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯\n",
    "# ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ù‚ÙŠØ§Ø³Ù‡\n",
    "epoch= 50\n",
    "\n",
    "# gradient descent\n",
    "for k in range(epoch):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=38).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.1*(W**2).mean()\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad\n",
    "    \n",
    "  if k % (epoch/10) == 0:\n",
    "    print(k,'epoch' ,loss.item())\n",
    "\n",
    "\n",
    "end_time = time.time()  # ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø¨Ø¹Ø¯ ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯\n",
    "total_time = end_time - start_time  # Ø­Ø³Ø§Ø¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°\n",
    "\n",
    "print(\"Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°:\", total_time, \"Ø«Ø§Ù†ÙŠØ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù†.\n",
      "Ù‚Ø©.\n",
      "Ù….\n",
      "ØªØ§Ù„Ø£Ø±Ø©.\n",
      "ØªØµ.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=38).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
